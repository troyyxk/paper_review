# 文章链接
https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e   

# Cloud TPU Multislice Training
1. Robust orchestration and scalability: Scale large model training across tens of thousands of TPU chips in a reliable and fault-tolerant way across the training workflow.
2. Performant compilation:Maximize performance and efficiency using the XLA compiler to automatically manage compute and communication.
3. Flexible stack for end-to-end training: Provide first-class support for popular ML frameworks such as JAX and PyTorch, easy-to-use reference implementations and libraries, and support for a wide range of model architectures including LLMs, diffusion models and DLRM.

pod IP exhaustion, Domain Name Service (DNS) scalability, and control-plane node limits
